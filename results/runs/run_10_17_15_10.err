2022-10-17 15:12:11,426	INFO services.py:1338 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
INFO:ray.worker:Using address auto set in the environment variable RAY_ADDRESS
2022-10-17 15:12:32,987	INFO worker.py:842 -- Connecting to existing Ray cluster at address: 100.97.71.163:6888
[2022-10-17 15:12:33,007 I 3529787 3529787] logging.cc:191: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1
DEBUG:compiler_gym.service.connection:Exec `./example_service.py --working_dir=/dev/shm/compiler_gym_dejang/s/1017T151234-315371-584b --alsologtostderr -v=1 --logbuflevel=-1`
I1017 15:12:38.158170 140570062312256 create_and_run_compiler_gym_service.py:117] Service /dev/shm/compiler_gym_dejang/s/1017T151234-315371-584b listening on 42181, PID = 3532068
I1017 15:12:38.317744 140568147511040 compiler_gym_service.py:105] GetSpaces()
DEBUG:compiler_gym.service.client_service_compiler_env:Setting benchmark: benchmark://mm32_8_16_8_4_16-v0/012345
I1017 15:12:38.342521 140570062312256 create_and_run_compiler_gym_service.py:48] Service received signal: 15
I1017 15:12:38.342996 140570062312256 create_and_run_compiler_gym_service.py:130] Shutting down the RPC service
I1017 15:12:38.343892 140570062312256 create_and_run_compiler_gym_service.py:133] Service closed
/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/site-packages/ray/tune/callback.py:217: FutureWarning: Please update `setup` method in callback `<class 'ray.tune.integration.wandb.WandbLoggerCallback'>` to match the method signature in `ray.tune.callback.Callback`.
  warnings.warn(
[2m[33m(raylet)[0m [2022-10-17 15:12:40,997 W 3532260 3532260] logging.cc:189: Unrecognized setting of RAY_BACKEND_LOG_LEVEL=0
[2m[33m(raylet)[0m [2022-10-17 15:12:40,997 I 3532260 3532260] logging.cc:191: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to 0
[2m[33m(raylet)[0m [2022-10-17 15:12:40,998 W 3532259 3532259] logging.cc:189: Unrecognized setting of RAY_BACKEND_LOG_LEVEL=0
[2m[33m(raylet)[0m [2022-10-17 15:12:40,998 I 3532259 3532259] logging.cc:191: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to 0
DEBUG:git.cmd:Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/private/home/dejang/tools/cg_rllib_slurm, universal_newlines=False, shell=None, istream=None)
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.wandb.ai:443
DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 521
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.wandb.ai:443
DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 325
wandb: Currently logged in as: dejang. Use `wandb login --relogin` to force relogin
DEBUG:git.cmd:Popen(['git', 'cat-file', '--batch-check'], cwd=/private/home/dejang/tools/cg_rllib_slurm, universal_newlines=False, shell=None, istream=<valid stream>)
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /private/home/dejang/tools/cg_rllib_slurm/wandb/run-20221017_151244-cff02_00000
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PPO_loop_tool_env-v0_cff02_00000
wandb: ‚≠êÔ∏è View project at https://wandb.ai/dejang/loop_tool_agent
wandb: üöÄ View run at https://wandb.ai/dejang/loop_tool_agent/runs/cff02_00000
[2m[36m(PPO pid=3532260)[0m 2022-10-17 15:12:50,221	INFO trainer.py:722 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also want to then set `eager_tracing=True` in order to reach similar execution speed as with static-graph mode.
[2m[36m(PPO pid=3532260)[0m 2022-10-17 15:12:50,222	INFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.
[2m[36m(PPO pid=3532260)[0m 2022-10-17 15:12:50,222	INFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(RolloutWorker pid=3532259)[0m E1017 15:13:02.855798 140615092958976 example_service.py:284] CRITICAL - 
[2m[36m(RolloutWorker pid=3532259)[0m 
[2m[36m(RolloutWorker pid=3532259)[0m Working_dir = /dev/shm/compiler_gym_dejang/s/1017T151259-581897-6f35
[2m[36m(RolloutWorker pid=3532259)[0m 
[2m[36m(RolloutWorker pid=3532259)[0m 2022-10-17 15:13:03,767	WARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!
[2m[36m(PPO pid=3532260)[0m 2022-10-17 15:13:04,965	WARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!
[2m[36m(PPO pid=3532260)[0m 2022-10-17 15:13:05,766	INFO trainable.py:124 -- Trainable.setup took 16.691 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(PPO pid=3532260)[0m 2022-10-17 15:13:05,767	WARNING util.py:57 -- Install gputil for GPU system monitoring.
[2m[36m(RolloutWorker pid=3532259)[0m E1017 15:13:05.805948 140615092958976 example_service.py:284] CRITICAL - 
[2m[36m(RolloutWorker pid=3532259)[0m 
[2m[36m(RolloutWorker pid=3532259)[0m Working_dir = /dev/shm/compiler_gym_dejang/s/1017T151259-581897-6f35
[2m[36m(RolloutWorker pid=3532259)[0m 
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.014 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: | 0.014 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                                      agent_timesteps_total ‚ñÅ
wandb:                                         episodes_this_iter ‚ñÅ
wandb:                                             episodes_total ‚ñÅ
wandb:     info/learner/default_policy/learner_stats/cur_kl_coeff ‚ñÅ
wandb:           info/learner/default_policy/learner_stats/cur_lr ‚ñÅ
wandb:          info/learner/default_policy/learner_stats/entropy ‚ñÅ
wandb:    info/learner/default_policy/learner_stats/entropy_coeff ‚ñÅ
wandb:               info/learner/default_policy/learner_stats/kl ‚ñÅ
wandb:      info/learner/default_policy/learner_stats/policy_loss ‚ñÅ
wandb:       info/learner/default_policy/learner_stats/total_loss ‚ñÅ
wandb: info/learner/default_policy/learner_stats/vf_explained_var ‚ñÅ
wandb:          info/learner/default_policy/learner_stats/vf_loss ‚ñÅ
wandb:                               info/num_agent_steps_sampled ‚ñÅ
wandb:                               info/num_agent_steps_trained ‚ñÅ
wandb:                                     info/num_steps_sampled ‚ñÅ
wandb:                                     info/num_steps_trained ‚ñÅ
wandb:                                   iterations_since_restore ‚ñÅ
wandb:                                        num_healthy_workers ‚ñÅ
wandb:                                      perf/cpu_util_percent ‚ñÅ
wandb:                                      perf/ram_util_percent ‚ñÅ
wandb:                                         time_since_restore ‚ñÅ
wandb:                                           time_this_iter_s ‚ñÅ
wandb:                                               time_total_s ‚ñÅ
wandb:                                    timers/learn_throughput ‚ñÅ
wandb:                                       timers/learn_time_ms ‚ñÅ
wandb:                                     timers/load_throughput ‚ñÅ
wandb:                                        timers/load_time_ms ‚ñÅ
wandb:                                   timers/sample_throughput ‚ñÅ
wandb:                                      timers/sample_time_ms ‚ñÅ
wandb:                                      timers/update_time_ms ‚ñÅ
wandb:                                                  timestamp ‚ñÅ
wandb:                                    timesteps_since_restore ‚ñÅ
wandb:                                        timesteps_this_iter ‚ñÅ
wandb:                                            timesteps_total ‚ñÅ
wandb:                                         training_iteration ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                                      agent_timesteps_total 5
wandb:                                           episode_len_mean nan
wandb:                                         episode_reward_max nan
wandb:                                        episode_reward_mean nan
wandb:                                         episode_reward_min nan
wandb:                                         episodes_this_iter 0
wandb:                                             episodes_total 0
wandb:     info/learner/default_policy/learner_stats/cur_kl_coeff 0.2
wandb:           info/learner/default_policy/learner_stats/cur_lr 5e-05
wandb:          info/learner/default_policy/learner_stats/entropy 1.38374
wandb:    info/learner/default_policy/learner_stats/entropy_coeff 0.0
wandb:               info/learner/default_policy/learner_stats/kl 0.00269
wandb:      info/learner/default_policy/learner_stats/policy_loss -0.06673
wandb:       info/learner/default_policy/learner_stats/total_loss -0.06612
wandb: info/learner/default_policy/learner_stats/vf_explained_var 0.78463
wandb:          info/learner/default_policy/learner_stats/vf_loss 7e-05
wandb:                               info/num_agent_steps_sampled 5
wandb:                               info/num_agent_steps_trained 5
wandb:                                     info/num_steps_sampled 5
wandb:                                     info/num_steps_trained 5
wandb:                                   iterations_since_restore 1
wandb:                                        num_healthy_workers 1
wandb:                                      perf/cpu_util_percent 15.4
wandb:                                      perf/ram_util_percent 11.5
wandb:                                         time_since_restore 0.64575
wandb:                                           time_this_iter_s 0.64575
wandb:                                               time_total_s 0.64575
wandb:                                    timers/learn_throughput 10.978
wandb:                                       timers/learn_time_ms 455.469
wandb:                                     timers/load_throughput 25826.995
wandb:                                        timers/load_time_ms 0.194
wandb:                                   timers/sample_throughput 26.76
wandb:                                      timers/sample_time_ms 186.846
wandb:                                      timers/update_time_ms 5.038
wandb:                                                  timestamp 1666044786
wandb:                                    timesteps_since_restore 0
wandb:                                        timesteps_this_iter 0
wandb:                                            timesteps_total 5
wandb:                                         training_iteration 1
wandb: 
wandb: Synced PPO_loop_tool_env-v0_cff02_00000: https://wandb.ai/dejang/loop_tool_agent/runs/cff02_00000
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221017_151244-cff02_00000/logs
2022-10-17 15:13:11,892	INFO tune.py:626 -- Total run time: 32.97 seconds (32.10 seconds for the tuning loop).
2022-10-17 15:13:11,914	WARNING experiment_analysis.py:510 -- Could not find best trial. Did you pass the correct `metric` parameter?
