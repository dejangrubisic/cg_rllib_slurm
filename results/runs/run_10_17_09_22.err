2022-10-17 09:24:22,082	INFO services.py:1338 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
INFO:ray.worker:Using address auto set in the environment variable RAY_ADDRESS
2022-10-17 09:24:46,792	INFO worker.py:842 -- Connecting to existing Ray cluster at address: 100.97.65.167:6888
[2022-10-17 09:24:46,813 I 457095 457095] logging.cc:191: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1
/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/site-packages/ray/tune/callback.py:217: FutureWarning: Please update `setup` method in callback `<class 'ray.tune.integration.wandb.WandbLoggerCallback'>` to match the method signature in `ray.tune.callback.Callback`.
  warnings.warn(
DEBUG:git.cmd:Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/private/home/dejang/tools/cg_rllib_slurm, universal_newlines=False, shell=None, istream=None)
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.wandb.ai:443
DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 521
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.wandb.ai:443
DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 325
wandb: Currently logged in as: dejang. Use `wandb login --relogin` to force relogin
wandb: WARNING Tried to auto resume run with id 18e14_00000 but id 376c7_00000 is set.
DEBUG:git.cmd:Popen(['git', 'cat-file', '--batch-check'], cwd=/private/home/dejang/tools/cg_rllib_slurm, universal_newlines=False, shell=None, istream=<valid stream>)
[2m[36m(PPO pid=459410)[0m 2022-10-17 09:24:59,677	INFO trainer.py:722 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also want to then set `eager_tracing=True` in order to reach similar execution speed as with static-graph mode.
[2m[36m(PPO pid=459410)[0m 2022-10-17 09:24:59,677	INFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.
[2m[36m(PPO pid=459410)[0m 2022-10-17 09:24:59,677	INFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.3
wandb: Run data is saved locally in /private/home/dejang/tools/cg_rllib_slurm/wandb/run-20221017_092454-376c7_00000
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PPO_compiler_gym_376c7_00000
wandb: ‚≠êÔ∏è View project at https://wandb.ai/dejang/loop_tool_agent
wandb: üöÄ View run at https://wandb.ai/dejang/loop_tool_agent/runs/376c7_00000
[2m[36m(RolloutWorker pid=459409)[0m 2022-10-17 09:25:09,049	WARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!
[2m[36m(PPO pid=459410)[0m 2022-10-17 09:25:10,313	WARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!
[2m[36m(PPO pid=459410)[0m 2022-10-17 09:25:11,086	INFO trainable.py:124 -- Trainable.setup took 12.399 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(PPO pid=459410)[0m 2022-10-17 09:25:11,086	WARNING util.py:57 -- Install gputil for GPU system monitoring.
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: - 0.007 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: \ 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: | 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: / 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: - 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: \ 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: | 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                      agent_timesteps_total ‚ñÅ
wandb:                                           episode_len_mean ‚ñÅ
wandb:                                         episode_reward_max ‚ñÅ
wandb:                                        episode_reward_mean ‚ñÅ
wandb:                                         episode_reward_min ‚ñÅ
wandb:                                         episodes_this_iter ‚ñÅ
wandb:                                             episodes_total ‚ñÅ
wandb:     info/learner/default_policy/learner_stats/cur_kl_coeff ‚ñÅ
wandb:           info/learner/default_policy/learner_stats/cur_lr ‚ñÅ
wandb:          info/learner/default_policy/learner_stats/entropy ‚ñÅ
wandb:    info/learner/default_policy/learner_stats/entropy_coeff ‚ñÅ
wandb:               info/learner/default_policy/learner_stats/kl ‚ñÅ
wandb:      info/learner/default_policy/learner_stats/policy_loss ‚ñÅ
wandb:       info/learner/default_policy/learner_stats/total_loss ‚ñÅ
wandb: info/learner/default_policy/learner_stats/vf_explained_var ‚ñÅ
wandb:          info/learner/default_policy/learner_stats/vf_loss ‚ñÅ
wandb:                               info/num_agent_steps_sampled ‚ñÅ
wandb:                               info/num_agent_steps_trained ‚ñÅ
wandb:                                     info/num_steps_sampled ‚ñÅ
wandb:                                     info/num_steps_trained ‚ñÅ
wandb:                                   iterations_since_restore ‚ñÅ
wandb:                                        num_healthy_workers ‚ñÅ
wandb:                                      perf/cpu_util_percent ‚ñÅ
wandb:                                      perf/ram_util_percent ‚ñÅ
wandb:                     sampler_perf/mean_action_processing_ms ‚ñÅ
wandb:                            sampler_perf/mean_env_render_ms ‚ñÅ
wandb:                              sampler_perf/mean_env_wait_ms ‚ñÅ
wandb:                             sampler_perf/mean_inference_ms ‚ñÅ
wandb:                    sampler_perf/mean_raw_obs_processing_ms ‚ñÅ
wandb:                                         time_since_restore ‚ñÅ
wandb:                                           time_this_iter_s ‚ñÅ
wandb:                                               time_total_s ‚ñÅ
wandb:                                    timers/learn_throughput ‚ñÅ
wandb:                                       timers/learn_time_ms ‚ñÅ
wandb:                                     timers/load_throughput ‚ñÅ
wandb:                                        timers/load_time_ms ‚ñÅ
wandb:                                   timers/sample_throughput ‚ñÅ
wandb:                                      timers/sample_time_ms ‚ñÅ
wandb:                                      timers/update_time_ms ‚ñÅ
wandb:                                                  timestamp ‚ñÅ
wandb:                                    timesteps_since_restore ‚ñÅ
wandb:                                        timesteps_this_iter ‚ñÅ
wandb:                                            timesteps_total ‚ñÅ
wandb:                                         training_iteration ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                                      agent_timesteps_total 5
wandb:                                           episode_len_mean 5.0
wandb:                                         episode_reward_max 0.63758
wandb:                                        episode_reward_mean 0.63758
wandb:                                         episode_reward_min 0.63758
wandb:                                         episodes_this_iter 1
wandb:                                             episodes_total 1
wandb:     info/learner/default_policy/learner_stats/cur_kl_coeff 0.2
wandb:           info/learner/default_policy/learner_stats/cur_lr 5e-05
wandb:          info/learner/default_policy/learner_stats/entropy 2.70628
wandb:    info/learner/default_policy/learner_stats/entropy_coeff 0.0
wandb:               info/learner/default_policy/learner_stats/kl 0.00158
wandb:      info/learner/default_policy/learner_stats/policy_loss -0.09375
wandb:       info/learner/default_policy/learner_stats/total_loss 0.06526
wandb: info/learner/default_policy/learner_stats/vf_explained_var 0.02419
wandb:          info/learner/default_policy/learner_stats/vf_loss 0.15869
wandb:                               info/num_agent_steps_sampled 5
wandb:                               info/num_agent_steps_trained 5
wandb:                                     info/num_steps_sampled 5
wandb:                                     info/num_steps_trained 5
wandb:                                   iterations_since_restore 1
wandb:                                        num_healthy_workers 1
wandb:                                      perf/cpu_util_percent 34.15
wandb:                                      perf/ram_util_percent 13.1
wandb:                     sampler_perf/mean_action_processing_ms 0.10582
wandb:                            sampler_perf/mean_env_render_ms 0.0
wandb:                              sampler_perf/mean_env_wait_ms 31.79085
wandb:                             sampler_perf/mean_inference_ms 6.59402
wandb:                    sampler_perf/mean_raw_obs_processing_ms 32.30731
wandb:                                         time_since_restore 0.78007
wandb:                                           time_this_iter_s 0.78007
wandb:                                               time_total_s 0.78007
wandb:                                    timers/learn_throughput 15.112
wandb:                                       timers/learn_time_ms 330.855
wandb:                                     timers/load_throughput 24643.384
wandb:                                        timers/load_time_ms 0.203
wandb:                                   timers/sample_throughput 10.486
wandb:                                      timers/sample_time_ms 476.833
wandb:                                      timers/update_time_ms 3.247
wandb:                                                  timestamp 1666023911
wandb:                                    timesteps_since_restore 0
wandb:                                        timesteps_this_iter 0
wandb:                                            timesteps_total 5
wandb:                                         training_iteration 1
wandb: 
wandb: Synced PPO_compiler_gym_376c7_00000: https://wandb.ai/dejang/loop_tool_agent/runs/376c7_00000
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221017_092454-376c7_00000/logs
[2m[36m(RolloutWorker pid=459409)[0m 2022-10-17 09:25:15,378	ERROR worker.py:431 -- SystemExit was raised from the worker
[2m[36m(RolloutWorker pid=459409)[0m Traceback (most recent call last):
[2m[36m(RolloutWorker pid=459409)[0m   File "python/ray/_raylet.pyx", line 625, in ray._raylet.execute_task
[2m[36m(RolloutWorker pid=459409)[0m   File "python/ray/_raylet.pyx", line 629, in ray._raylet.execute_task
[2m[36m(RolloutWorker pid=459409)[0m   File "python/ray/_raylet.pyx", line 578, in ray._raylet.execute_task.function_executor
[2m[36m(RolloutWorker pid=459409)[0m   File "/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/site-packages/ray/_private/function_manager.py", line 609, in actor_method_executor
[2m[36m(RolloutWorker pid=459409)[0m     return method(__ray_actor, *args, **kwargs)
[2m[36m(RolloutWorker pid=459409)[0m   File "/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/site-packages/ray/util/tracing/tracing_helper.py", line 451, in _resume_span
[2m[36m(RolloutWorker pid=459409)[0m     return method(self, *_args, **_kwargs)
[2m[36m(RolloutWorker pid=459409)[0m   File "/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/site-packages/ray/actor.py", line 1081, in __ray_terminate__
[2m[36m(RolloutWorker pid=459409)[0m     ray.actor.exit_actor()
[2m[36m(RolloutWorker pid=459409)[0m   File "/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/site-packages/ray/actor.py", line 1160, in exit_actor
[2m[36m(RolloutWorker pid=459409)[0m     raise exit
[2m[36m(RolloutWorker pid=459409)[0m SystemExit: 0
[2m[36m(RolloutWorker pid=459409)[0m 
[2m[36m(RolloutWorker pid=459409)[0m During handling of the above exception, another exception occurred:
[2m[36m(RolloutWorker pid=459409)[0m 
[2m[36m(RolloutWorker pid=459409)[0m Traceback (most recent call last):
[2m[36m(RolloutWorker pid=459409)[0m   File "python/ray/_raylet.pyx", line 759, in ray._raylet.task_execution_handler
[2m[36m(RolloutWorker pid=459409)[0m   File "python/ray/_raylet.pyx", line 580, in ray._raylet.execute_task
[2m[36m(RolloutWorker pid=459409)[0m   File "python/ray/_raylet.pyx", line 618, in ray._raylet.execute_task
[2m[36m(RolloutWorker pid=459409)[0m   File "python/ray/includes/libcoreworker.pxi", line 33, in ray._raylet.ProfileEvent.__exit__
[2m[36m(RolloutWorker pid=459409)[0m   File "/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/traceback.py", line 167, in format_exc
[2m[36m(RolloutWorker pid=459409)[0m     return "".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))
[2m[36m(RolloutWorker pid=459409)[0m   File "/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/traceback.py", line 120, in format_exception
[2m[36m(RolloutWorker pid=459409)[0m     return list(TracebackException(
[2m[36m(RolloutWorker pid=459409)[0m   File "/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/traceback.py", line 508, in __init__
[2m[36m(RolloutWorker pid=459409)[0m     self.stack = StackSummary.extract(
[2m[36m(RolloutWorker pid=459409)[0m   File "/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/traceback.py", line 366, in extract
[2m[36m(RolloutWorker pid=459409)[0m     f.line
[2m[36m(RolloutWorker pid=459409)[0m   File "/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/traceback.py", line 288, in line
[2m[36m(RolloutWorker pid=459409)[0m     self._line = linecache.getline(self.filename, self.lineno).strip()
[2m[36m(RolloutWorker pid=459409)[0m   File "/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/linecache.py", line 16, in getline
[2m[36m(RolloutWorker pid=459409)[0m     lines = getlines(filename, module_globals)
[2m[36m(RolloutWorker pid=459409)[0m   File "/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/linecache.py", line 47, in getlines
[2m[36m(RolloutWorker pid=459409)[0m     return updatecache(filename, module_globals)
[2m[36m(RolloutWorker pid=459409)[0m   File "/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/linecache.py", line 95, in updatecache
[2m[36m(RolloutWorker pid=459409)[0m     stat = os.stat(fullname)
[2m[36m(RolloutWorker pid=459409)[0m   File "/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/site-packages/ray/worker.py", line 428, in sigterm_handler
[2m[36m(RolloutWorker pid=459409)[0m     sys.exit(1)
[2m[36m(RolloutWorker pid=459409)[0m SystemExit: 1
2022-10-17 09:25:15,468	INFO tune.py:626 -- Total run time: 28.47 seconds (27.73 seconds for the tuning loop).
2022-10-17 09:25:15,490	WARNING experiment_analysis.py:510 -- Could not find best trial. Did you pass the correct `metric` parameter?
