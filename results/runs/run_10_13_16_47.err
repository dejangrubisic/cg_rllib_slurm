2022-10-13 16:59:42,451	INFO services.py:1338 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
INFO:ray.worker:Using address auto set in the environment variable RAY_ADDRESS
2022-10-13 17:00:08,354	INFO worker.py:842 -- Connecting to existing Ray cluster at address: 100.97.65.203:6888
[2022-10-13 17:00:08,366 I 3884901 3884901] logging.cc:191: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1
/private/home/dejang/.conda/envs/compiler_gym/lib/python3.8/site-packages/ray/tune/callback.py:217: FutureWarning: Please update `setup` method in callback `<class 'ray.tune.integration.wandb.WandbLoggerCallback'>` to match the method signature in `ray.tune.callback.Callback`.
  warnings.warn(
DEBUG:git.cmd:Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/private/home/dejang/tools/cg_rllib_slurm, universal_newlines=False, shell=None, istream=None)
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.wandb.ai:443
DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 521
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.wandb.ai:443
DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 325
wandb: Currently logged in as: dejang. Use `wandb login --relogin` to force relogin
DEBUG:git.cmd:Popen(['git', 'cat-file', '--batch-check'], cwd=/private/home/dejang/tools/cg_rllib_slurm, universal_newlines=False, shell=None, istream=<valid stream>)
[2m[36m(PPO pid=3885505)[0m 2022-10-13 17:00:19,228	INFO trainer.py:722 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also want to then set `eager_tracing=True` in order to reach similar execution speed as with static-graph mode.
[2m[36m(PPO pid=3885505)[0m 2022-10-13 17:00:19,228	INFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.
[2m[36m(PPO pid=3885505)[0m 2022-10-13 17:00:19,228	INFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.3
wandb: Run data is saved locally in /private/home/dejang/tools/cg_rllib_slurm/wandb/run-20221013_170013-2a92a_00000
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PPO_compiler_gym_2a92a_00000
wandb: ‚≠êÔ∏è View project at https://wandb.ai/dejang/loop_tool_agent
wandb: üöÄ View run at https://wandb.ai/dejang/loop_tool_agent/runs/2a92a_00000
[2m[36m(RolloutWorker pid=3885504)[0m 2022-10-13 17:00:27,831	WARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!
[2m[36m(PPO pid=3885505)[0m 2022-10-13 17:00:28,956	WARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!
[2m[36m(PPO pid=3885505)[0m 2022-10-13 17:00:29,634	INFO trainable.py:124 -- Trainable.setup took 11.357 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(PPO pid=3885505)[0m 2022-10-13 17:00:29,634	WARNING util.py:57 -- Install gputil for GPU system monitoring.
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: - 0.014 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: \ 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: | 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: / 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: - 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: \ 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb: | 0.022 MB of 0.022 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                      agent_timesteps_total ‚ñÅ
wandb:                                           episode_len_mean ‚ñÅ
wandb:                                         episode_reward_max ‚ñÅ
wandb:                                        episode_reward_mean ‚ñÅ
wandb:                                         episode_reward_min ‚ñÅ
wandb:                                         episodes_this_iter ‚ñÅ
wandb:                                             episodes_total ‚ñÅ
wandb:     info/learner/default_policy/learner_stats/cur_kl_coeff ‚ñÅ
wandb:           info/learner/default_policy/learner_stats/cur_lr ‚ñÅ
wandb:          info/learner/default_policy/learner_stats/entropy ‚ñÅ
wandb:    info/learner/default_policy/learner_stats/entropy_coeff ‚ñÅ
wandb:               info/learner/default_policy/learner_stats/kl ‚ñÅ
wandb:      info/learner/default_policy/learner_stats/policy_loss ‚ñÅ
wandb:       info/learner/default_policy/learner_stats/total_loss ‚ñÅ
wandb: info/learner/default_policy/learner_stats/vf_explained_var ‚ñÅ
wandb:          info/learner/default_policy/learner_stats/vf_loss ‚ñÅ
wandb:                               info/num_agent_steps_sampled ‚ñÅ
wandb:                               info/num_agent_steps_trained ‚ñÅ
wandb:                                     info/num_steps_sampled ‚ñÅ
wandb:                                     info/num_steps_trained ‚ñÅ
wandb:                                   iterations_since_restore ‚ñÅ
wandb:                                        num_healthy_workers ‚ñÅ
wandb:                                      perf/cpu_util_percent ‚ñÅ
wandb:                                      perf/ram_util_percent ‚ñÅ
wandb:                     sampler_perf/mean_action_processing_ms ‚ñÅ
wandb:                            sampler_perf/mean_env_render_ms ‚ñÅ
wandb:                              sampler_perf/mean_env_wait_ms ‚ñÅ
wandb:                             sampler_perf/mean_inference_ms ‚ñÅ
wandb:                    sampler_perf/mean_raw_obs_processing_ms ‚ñÅ
wandb:                                         time_since_restore ‚ñÅ
wandb:                                           time_this_iter_s ‚ñÅ
wandb:                                               time_total_s ‚ñÅ
wandb:                                    timers/learn_throughput ‚ñÅ
wandb:                                       timers/learn_time_ms ‚ñÅ
wandb:                                     timers/load_throughput ‚ñÅ
wandb:                                        timers/load_time_ms ‚ñÅ
wandb:                                   timers/sample_throughput ‚ñÅ
wandb:                                      timers/sample_time_ms ‚ñÅ
wandb:                                      timers/update_time_ms ‚ñÅ
wandb:                                                  timestamp ‚ñÅ
wandb:                                    timesteps_since_restore ‚ñÅ
wandb:                                        timesteps_this_iter ‚ñÅ
wandb:                                            timesteps_total ‚ñÅ
wandb:                                         training_iteration ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                                      agent_timesteps_total 5
wandb:                                           episode_len_mean 5.0
wandb:                                         episode_reward_max 0.63758
wandb:                                        episode_reward_mean 0.63758
wandb:                                         episode_reward_min 0.63758
wandb:                                         episodes_this_iter 1
wandb:                                             episodes_total 1
wandb:     info/learner/default_policy/learner_stats/cur_kl_coeff 0.2
wandb:           info/learner/default_policy/learner_stats/cur_lr 5e-05
wandb:          info/learner/default_policy/learner_stats/entropy 2.70628
wandb:    info/learner/default_policy/learner_stats/entropy_coeff 0.0
wandb:               info/learner/default_policy/learner_stats/kl 0.00158
wandb:      info/learner/default_policy/learner_stats/policy_loss -0.09375
wandb:       info/learner/default_policy/learner_stats/total_loss 0.06526
wandb: info/learner/default_policy/learner_stats/vf_explained_var 0.02419
wandb:          info/learner/default_policy/learner_stats/vf_loss 0.15869
wandb:                               info/num_agent_steps_sampled 5
wandb:                               info/num_agent_steps_trained 5
wandb:                                     info/num_steps_sampled 5
wandb:                                     info/num_steps_trained 5
wandb:                                   iterations_since_restore 1
wandb:                                        num_healthy_workers 1
wandb:                                      perf/cpu_util_percent 4.0
wandb:                                      perf/ram_util_percent 3.8
wandb:                     sampler_perf/mean_action_processing_ms 0.09104
wandb:                            sampler_perf/mean_env_render_ms 0.0
wandb:                              sampler_perf/mean_env_wait_ms 22.82222
wandb:                             sampler_perf/mean_inference_ms 5.69975
wandb:                    sampler_perf/mean_raw_obs_processing_ms 21.80088
wandb:                                         time_since_restore 0.65823
wandb:                                           time_this_iter_s 0.65823
wandb:                                               time_total_s 0.65823
wandb:                                    timers/learn_throughput 15.643
wandb:                                       timers/learn_time_ms 319.627
wandb:                                     timers/load_throughput 30795.184
wandb:                                        timers/load_time_ms 0.162
wandb:                                   timers/sample_throughput 14.921
wandb:                                      timers/sample_time_ms 335.096
wandb:                                      timers/update_time_ms 2.245
wandb:                                                  timestamp 1665705630
wandb:                                    timesteps_since_restore 0
wandb:                                        timesteps_this_iter 0
wandb:                                            timesteps_total 5
wandb:                                         training_iteration 1
wandb: 
wandb: Synced PPO_compiler_gym_2a92a_00000: https://wandb.ai/dejang/loop_tool_agent/runs/2a92a_00000
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221013_170013-2a92a_00000/logs
2022-10-13 17:00:34,000	INFO tune.py:626 -- Total run time: 25.46 seconds (24.94 seconds for the tuning loop).
2022-10-13 17:00:34,016	WARNING experiment_analysis.py:510 -- Could not find best trial. Did you pass the correct `metric` parameter?
